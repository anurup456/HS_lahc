{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2Vp9PfmIzed",
        "outputId": "a141390f-5fa7-446e-9cca-841d70856a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting skfeature-chappers\n",
            "  Downloading skfeature_chappers-1.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from skfeature-chappers) (1.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from skfeature-chappers) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from skfeature-chappers) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->skfeature-chappers) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->skfeature-chappers) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->skfeature-chappers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->skfeature-chappers) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->skfeature-chappers) (1.10.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->skfeature-chappers) (1.15.0)\n",
            "Installing collected packages: skfeature-chappers\n",
            "Successfully installed skfeature-chappers-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ReliefF\n",
            "  Downloading ReliefF-0.1.2.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ReliefF) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from ReliefF) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from ReliefF) (1.2.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->ReliefF) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->ReliefF) (3.1.0)\n",
            "Building wheels for collected packages: ReliefF\n",
            "  Building wheel for ReliefF (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ReliefF: filename=ReliefF-0.1.2-py3-none-any.whl size=6318 sha256=bad69699655e5bb99631777e99d13367b6086b942db11c5c66de459acf0166be\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/90/ef/7691706c1c418d2c266a0b3b06371900f4b86e74223912c563\n",
            "Successfully built ReliefF\n",
            "Installing collected packages: ReliefF\n",
            "Successfully installed ReliefF-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install skfeature-chappers\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "from skfeature.function.similarity_based import fisher_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math,time,sys, os\n",
        "from matplotlib import pyplot\n",
        "from datetime import datetime\n",
        "\n",
        "!pip install ReliefF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STZ8tAsCLa2I",
        "outputId": "593d1533-7622-42be-c921-38a1d8cf22ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.17.3\n",
            "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.2.0 numpy-1.24.2 scikit-learn-1.2.1 scipy-1.10.1 threadpoolctl-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.24.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting skfeature-chappers\n",
            "  Using cached skfeature_chappers-1.1.0-py3-none-any.whl (66 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Using cached scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "Collecting python-dateutil>=2.8.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 KB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.3.2\n",
            "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, threadpoolctl, six, numpy, joblib, scipy, python-dateutil, scikit-learn, pandas, skfeature-chappers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.2.0 numpy-1.24.2 pandas-1.5.3 python-dateutil-2.8.2 pytz-2022.7.1 scikit-learn-1.2.1 scipy-1.10.1 six-1.16.0 skfeature-chappers-1.1.0 threadpoolctl-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (0.8.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ReliefF\n",
            "  Using cached ReliefF-0.1.2-py3-none-any.whl\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, ReliefF\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ReliefF-0.1.2 joblib-1.2.0 numpy-1.24.2 scikit-learn-1.2.1 scipy-1.10.1 threadpoolctl-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --ignore-installed scikit-learn\n",
        "!pip install --upgrade numpy\n",
        "!pip install --ignore-installed skfeature-chappers\n",
        "# !pip install --upgrade numpy\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "from skfeature.function.similarity_based import fisher_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math,time,sys, os\n",
        "from matplotlib import pyplot\n",
        "from datetime import datetime\n",
        "\n",
        "!pip install tabulate\n",
        "!pip install --ignore-installed ReliefF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrPXColcLrC6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Solution():    \n",
        "    #structure of the solution \n",
        "    def __init__(self):\n",
        "        self.num_features = None\n",
        "        self.num_agents = None\n",
        "        self.max_iter = None\n",
        "        self.obj_function = None\n",
        "        self.execution_time = None\n",
        "        self.convergence_curve = {}\n",
        "        self.best_agent = None\n",
        "        self.best_fitness = None\n",
        "        self.best_accuracy = None\n",
        "        self.final_population = None\n",
        "        self.final_fitness = None\n",
        "        self.final_accuracy = None\n",
        "\n",
        "\n",
        "class Data():\n",
        "    # structure of the training data\n",
        "    def __init__(self):\n",
        "        self.train_X = None\n",
        "        self.train_Y = None\n",
        "        self.val_X = None\n",
        "        self.val_Y = None\n",
        "\n",
        "\n",
        "\n",
        "def initialize(num_agents, num_features):\n",
        "    # define min and max number of features\n",
        "    min_features = int(0.3 * num_features)\n",
        "    max_features = int(0.6 * num_features)\n",
        "\n",
        "    # initialize the agents with zeros\n",
        "    agents = np.zeros((num_agents, num_features))\n",
        "\n",
        "    # select random features for each agent\n",
        "    for agent_no in range(num_agents):\n",
        "\n",
        "        # find random indices\n",
        "        cur_count = np.random.randint(min_features, max_features)\n",
        "        temp_vec = np.random.rand(1, num_features)\n",
        "        temp_idx = np.argsort(temp_vec)[0][0:cur_count]\n",
        "\n",
        "        # select the features with the ranom indices\n",
        "        agents[agent_no][temp_idx] = 1   \n",
        "\n",
        "    return agents\n",
        "\n",
        "\n",
        "\n",
        "def sort_agents(agents, obj, data, fitness=None):\n",
        "    # sort the agents according to fitness\n",
        "    train_X, val_X, train_Y, val_Y = data.train_X, data.val_X, data.train_Y, data.val_Y\n",
        "    (obj_function, weight_acc) = obj\n",
        "   \n",
        "    if fitness is None:\n",
        "        # if there is only one agent\n",
        "        if len(agents.shape) == 1:\n",
        "            num_agents = 1\n",
        "            fitness = obj_function(agents, train_X, val_X, train_Y, val_Y, weight_acc)\n",
        "            return agents, fitness\n",
        "\n",
        "        # for multiple agents\n",
        "        else:\n",
        "            num_agents = agents.shape[0]\n",
        "            fitness = np.zeros(num_agents)\n",
        "            for id, agent in enumerate(agents):\n",
        "                fitness[id] = obj_function(agent, train_X, val_X, train_Y, val_Y, weight_acc)\n",
        "\n",
        "    idx = np.argsort(-fitness)\n",
        "    sorted_agents = agents[idx].copy()\n",
        "    sorted_fitness = fitness[idx].copy()\n",
        "\n",
        "    return sorted_agents, sorted_fitness\n",
        "\n",
        "\n",
        "\n",
        "def display(agents, fitness, agent_name='Agent'):\n",
        "    # display the population\n",
        "    print('\\nNumber of agents: {}'.format(agents.shape[0]))\n",
        "    print('\\n------------- Best Agent ---------------')\n",
        "    print('Fitness: {}'.format(fitness[0]))\n",
        "    print('Number of Features: {}'.format(int(np.sum(agents[0]))))\n",
        "    print('----------------------------------------\\n')\n",
        "\n",
        "    for id, agent in enumerate(agents):\n",
        "        print('{} {} - Fitness: {}, Number of Features: {}'.format(agent_name, id+1, fitness[id], int(np.sum(agent))))\n",
        "\n",
        "    print('================================================================================\\n')\n",
        "\n",
        "\n",
        "\n",
        "def compute_accuracy(agent, train_X, test_X, train_Y, test_Y): \n",
        "    # compute classification accuracy of the given agents\n",
        "    cols = np.flatnonzero(agent)     \n",
        "    if(cols.shape[0] == 0):\n",
        "        return 0    \n",
        "    clf = KNN()\n",
        "\n",
        "    train_data = train_X[:,cols]\n",
        "    train_label = train_Y\n",
        "    test_data = test_X[:,cols]\n",
        "    test_label = test_Y\n",
        "\n",
        "    clf.fit(train_data,train_label)\n",
        "    acc = clf.score(test_data,test_label)\n",
        "\n",
        "    return acc\n",
        "        \n",
        "\n",
        "def compute_fitness(agent, train_X, test_X, train_Y, test_Y, weight_acc=0.9):\n",
        "    # compute a basic fitness measure\n",
        "    if(weight_acc == None):\n",
        "        weight_acc = 0.9\n",
        "\n",
        "    weight_feat = 1 - weight_acc\n",
        "    num_features = agent.shape[0]\n",
        "    \n",
        "    acc = compute_accuracy(agent, train_X, test_X, train_Y, test_Y)\n",
        "    feat = (num_features - np.sum(agent))/num_features\n",
        "\n",
        "    fitness = weight_acc * acc + weight_feat * feat\n",
        "    return fitness\n",
        "\n",
        "\n",
        "def Conv_plot(convergence_curve):\n",
        "    # plot convergence curves\n",
        "    num_iter = len(convergence_curve['fitness'])\n",
        "    iters = np.arange(num_iter) + 1\n",
        "    fig, axes = plt.subplots(1)\n",
        "    fig.tight_layout(pad = 5) \n",
        "    fig.suptitle('Convergence Curves')\n",
        "    \n",
        "    axes.set_title('Convergence of Fitness over Iterations')\n",
        "    axes.set_xlabel('Iteration')\n",
        "    axes.set_ylabel('Avg. Fitness')\n",
        "    axes.plot(iters, convergence_curve['fitness'])\n",
        "\n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH1C1Q5LLvtV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(val):\n",
        "    if val < 0:\n",
        "        return 1 - 1/(1 + np.exp(val))\n",
        "    else:\n",
        "        return 1/(1 + np.exp(-val))\n",
        "\n",
        "\n",
        "def v_func(val):\n",
        "    return abs(val/(np.sqrt(1 + val*val)))\n",
        "\n",
        "\n",
        "def u_func(val):\n",
        "    alpha, beta = 2, 1.5\n",
        "    return abs(alpha * np.power(abs(val), beta))\n",
        "\n",
        "\n",
        "def get_trans_function(shape):\n",
        "    if (shape.lower() == 's'):\n",
        "        return sigmoid\n",
        "\n",
        "    elif (shape.lower() == 'v'):\n",
        "        return v_func\n",
        "\n",
        "    elif(shape.lower() == 'u'):\n",
        "        return u_func\n",
        "\n",
        "    else:\n",
        "        print('\\n[Error!] We don\\'t currently support {}-shaped transfer functions...\\n'.format(shape))\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLEwVCz9j6dU"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "from skfeature.function.similarity_based import fisher_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math,time,sys, os\n",
        "from matplotlib import pyplot\n",
        "from datetime import datetime\n",
        "!pip install ReliefF\n",
        "predicted=[]\n",
        "\n",
        "def chaos_map(num_features):\n",
        "    firs=np.random.random()\n",
        "    logis_map=np.random.rand(1,num_features)\n",
        "    for i in range(num_features):\n",
        "        k=4*firs*(1-firs)\n",
        "        logis_map[0][i]=k\n",
        "        firs=k\n",
        "    return logis_map\n",
        "def initialize(num_agents, num_features):\n",
        "    # define min and max number of features\n",
        "    min_features = int(0.3 * num_features)\n",
        "    max_features = int(0.6 * num_features)\n",
        "\n",
        "    # initialize the agents with zeros\n",
        "    agents = np.zeros((num_agents, num_features))\n",
        "\n",
        "    # select random features for each agent\n",
        "    for agent_no in range(num_agents):\n",
        "\n",
        "        # find random indices\n",
        "        cur_count = np.random.randint(min_features, max_features)\n",
        "        temp_vec = chaos_map(num_features)\n",
        "        temp_idx = np.argsort(temp_vec)[0][0:cur_count]\n",
        "\n",
        "        # select the features with the ranom indices\n",
        "        agents[agent_no][temp_idx] = 1   \n",
        "\n",
        "    return agents\n",
        "\n",
        "def HS(num_agents, max_iter, train_data, train_label, obj_function = compute_fitness, save_conv_graph = False):\n",
        "    def mutate(agent):\n",
        "        percent=0.2\n",
        "        numChange=int(num_features*percent)\n",
        "        pos=np.random.randint(0,num_features-1,numChange) #choose random positions to be mutated\n",
        "        agent[pos]=1-agent[pos] #mutation\n",
        "        return agent\n",
        "    def LAHC(particle,lamb,targ):\n",
        "                _lambda = lamb #upper limit on number of iterations in LAHC\n",
        "                target_fitness = compute_fitness(particle,data.train_X, data.val_X, data.train_Y, data.val_Y) #original fitness\n",
        "                for i in range(_lambda):\n",
        "                        new_particle = mutate(particle) #first mutation\n",
        "                        temp = compute_fitness(new_particle,data.train_X, data.val_X, data.train_Y, data.val_Y)\n",
        "                        if temp > target_fitness:\n",
        "                            particle = new_particle.copy() #updation\n",
        "                            target_fitness = temp\n",
        "                        elif (temp>=(1-targ)*target_fitness):\n",
        "                            temp_particle = new_particle.copy()\n",
        "                            for j in range(_lambda):\n",
        "                                temp_particle1 = mutate(temp_particle) #second mutation\n",
        "                                temp_fitness = compute_fitness(temp_particle1,data.train_X, data.val_X, data.train_Y, data.val_Y)\n",
        "                                if temp_fitness > target_fitness:\n",
        "                                    target_fitness=temp_fitness\n",
        "                                    particle=temp_particle1.copy() #updation\n",
        "                                    break\n",
        "                return particle \n",
        "    # Harmony Search Algorithm\n",
        "\n",
        "  #  _particle1,data.train_X, data.val_X, data.train_Y, data.val_Y)\n",
        "  #                               if temp_fitness < target_fitness:\n",
        "  #                                   target_fitness=temp_fitness\n",
        "  #                                   particle=temp_particle1.copy() #updation\n",
        "  #                                   break\n",
        "    #             return particle \n",
        "    # # Harmony Search Algorithm\n",
        "    ############################### Parameters ####################################\n",
        "    #                                                                             #\n",
        "    #   num_agents: number of harmonies                                           #\n",
        "    #   max_iter: maximum number of generations                                   #\n",
        "    #   train_data: training samples of data                                      #\n",
        "    #   train_label: class labels for the training samples                        #                \n",
        "    #   obj_function: the function to maximize while doing feature selection      #\n",
        "    #   trans_function_shape: shape of the transfer function used                 #\n",
        "    #   save_conv_graph: boolean value for saving convergence graph               #\n",
        "    #                                                                             #\n",
        "    ###############################################################################\n",
        "    #for a in [0.8,0.9,0.95]: b: for b in [10,15,20]:    or c in [0.1,0.2,0.3]:\n",
        "\n",
        "    # <STEPS OF HARMOMY SEARCH ALGORITH>\n",
        "    # Step 1. Initialize a Harmony Memory (HM).\n",
        "    # Step 2. Improvise a new harmony from HM.\n",
        "    # Step 3. If the new harmony is better than minimum harmony in HM, include the new harmony in HM, and exclude the minimum harmony from HM.\n",
        "    # Step 4. If stopping criteria are not satisfied, go to Step 2.\n",
        "    for a in [0.95]:\n",
        "        for b in [20]:\n",
        "            for c in [0.3]:\n",
        "                print(f'\\n Hi Guys welcome to HS+LAHC.BTW\\n HMCR: {a}\\n Lambda: {b}\\n Targ Fit: {c}\\n')\n",
        "                short_name = 'HS'\n",
        "                agent_name = 'Harmony'\n",
        "                train_data, train_label = np.array(train_data), np.array(train_label)\n",
        "                num_features = train_data.shape[1]\n",
        "\n",
        "                # setting up the objectives\n",
        "                weight_acc = None\n",
        "                if(obj_function==compute_fitness):\n",
        "                    # weight_acc = float(input('Weight for the classification accuracy [0-1]: '))\n",
        "                    weight_acc=0.95\n",
        "                obj = (obj_function, weight_acc)\n",
        "                compute_accuracy = (compute_fitness, 1) # compute_accuracy is just compute_fitness with accuracy weight as 1\n",
        "\n",
        "                # intialize the harmonies and Leader (the agent with the max fitness)\n",
        "                harmonyMemory = initialize(num_agents, num_features)\n",
        "                fitness = np.zeros(num_agents)\n",
        "                accuracy = np.zeros(num_agents)\n",
        "                Leader_agent = np.zeros((1, num_features))\n",
        "                Leader_fitness = float(\"-inf\")\n",
        "                Leader_accuracy = float(\"-inf\")\n",
        "                HMCR = a     # Harmony Memory Consideration Rate\n",
        "\n",
        "                # initialize convergence curves\n",
        "                convergence_curve = {}\n",
        "                convergence_curve['fitness'] = np.zeros(max_iter)\n",
        "\n",
        "                # initialize data class\n",
        "                data = Data()\n",
        "                # val_size = float(input('Enter the percentage of data wanted for valdiation [0, 100]: '))/100\n",
        "                val_size =0.2\n",
        "\n",
        "                data.train_X, data.val_X, data.train_Y, data.val_Y = train_test_split(train_data, train_label, stratify=train_label, test_size=val_size)\n",
        "\n",
        "                # create a solution object\n",
        "                solution = Solution()\n",
        "                solution.num_agents = num_agents\n",
        "                solution.max_iter = max_iter\n",
        "                solution.num_features = num_features\n",
        "                solution.obj_function = obj_function\n",
        "\n",
        "                # start timer\n",
        "                start_time = time.time()\n",
        "\n",
        "                # calculate initial fitess and sort the harmony memory and rank them\n",
        "                harmonyMemory, fitness = sort_agents(harmonyMemory, obj, data)\n",
        "                mean_best=0\n",
        "                # create new harmonies in each iteration\n",
        "                for iter_no in range(max_iter):\n",
        "                    print('\\n================================================================================')\n",
        "                    print('                          Iteration - {}'.format(iter_no + 1))\n",
        "                    print('================================================================================\\n')\n",
        "                    HMCR_randValue = np.random.rand()\n",
        "                    newHarmony = np.zeros([1, num_features])\n",
        "\n",
        "                    # print(HMCR)\n",
        "                    # print(HMCR_randValue)\n",
        "\n",
        "                    if HMCR_randValue <= HMCR:\n",
        "                        for featureNum in range(num_features):\n",
        "                            selectedAgent = random.randint(0, num_agents - 1)\n",
        "                            newHarmony[0, featureNum] = harmonyMemory[selectedAgent, featureNum]\n",
        "\n",
        "                    else:\n",
        "                        for featureNum in range(num_features):\n",
        "                            newHarmony[0, featureNum] = random.randint(0, 1)\n",
        "\n",
        "                    fitnessHarmony = obj_function(newHarmony, data.train_X, data.val_X, data.train_Y, data.val_Y)\n",
        "\n",
        "                    if fitness[num_agents-1] < fitnessHarmony:\n",
        "                        harmonyMemory[num_agents-1, :] = newHarmony\n",
        "                        fitness[num_agents-1] = fitnessHarmony\n",
        "                    for i in range(len(harmonyMemory)):\n",
        "                        harmonyMemory[i]=LAHC(harmonyMemory[i],b,c)\n",
        "\n",
        "                    # sort harmony memory\n",
        "                    harmonyMemory, fitness = sort_agents(harmonyMemory, obj, data)\n",
        "                    \n",
        "                    if fitness[0] > Leader_fitness:\n",
        "                        Leader_agent = harmonyMemory[0].copy()\n",
        "                        Leader_fitness = fitness[0].copy()\n",
        "                    if np.mean(fitness)> mean_best:\n",
        "                        mean_best=np.mean(fitness)\n",
        "\n",
        "                    # update \n",
        "                    convergence_curve['fitness'][iter_no] = mean_best\n",
        "                    # display(harmonyMemory, fitness, agent_name)\n",
        "                \n",
        "                # compute final accuracy\n",
        "                Leader_agent, Leader_accuracy = sort_agents(Leader_agent, compute_accuracy, data)\n",
        "                harmonyMemory, accuracy = sort_agents(harmonyMemory, compute_accuracy, data)\n",
        "\n",
        "                print('\\n================================================================================')\n",
        "                print('                                    Final Result                                  ')\n",
        "                print('================================================================================\\n')\n",
        "                print('Leader ' + agent_name + ' Dimension : {}'.format(int(np.sum(Leader_agent))))\n",
        "                print('Leader ' + agent_name + ' Fitness : {}'.format(Leader_fitness))\n",
        "                print('Leader ' + agent_name + ' Classification Accuracy : {}'.format(Leader_accuracy))\n",
        "                print('\\n================================================================================\\n')\n",
        "                # print(Leader_agent)\n",
        "\n",
        "                # leader agent and leader fitneess\n",
        "                Leader_fitness = fitness[0]\n",
        "                Leader_agent = harmonyMemory[0].copy()\n",
        "\n",
        "                # stop timer\n",
        "                end_time = time.time()\n",
        "                exec_time = end_time - start_time\n",
        "\n",
        "                # plot convergence graph\n",
        "                fig, axes = Conv_plot(convergence_curve)\n",
        "                if(save_conv_graph):\n",
        "                    plt.savefig('convergence_graph_'+ short_name + '.jpg')\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "                solution.best_agent = Leader_agent\n",
        "                solution.best_fitness = Leader_fitness\n",
        "                solution.best_accuracy = Leader_accuracy\n",
        "                solution.convergence_curve = convergence_curve\n",
        "                solution.final_population = harmonyMemory\n",
        "                solution.final_fitness = fitness\n",
        "                solution.final_accuracy = accuracy\n",
        "                solution.execution_time = exec_time\n",
        "\n",
        "\n",
        "    return solution\n",
        "\n",
        "\n",
        "\n",
        "#UCI\n",
        "\n",
        "dataset=''\n",
        "df=pd.read_csv(dataset)\n",
        "(a,b)=np.shape(df)\n",
        "data = df.values[:,0:b-1]\n",
        "label = df.values[:,b-1]\n",
        "\n",
        "it=3\n",
        "while it>0:\n",
        "  predicted=HS(35, 60, data, label, save_conv_graph=True)\n",
        "  it=it-1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VaOdka4_OpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}